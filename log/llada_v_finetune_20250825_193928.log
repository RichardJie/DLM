[2025-08-25 19:39:36,204] torch.distributed.run: [WARNING] 
[2025-08-25 19:39:36,204] torch.distributed.run: [WARNING] *****************************************
[2025-08-25 19:39:36,204] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-08-25 19:39:36,204] torch.distributed.run: [WARNING] *****************************************
[2025-08-25 19:39:40,864] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 19:39:40,864] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 19:39:40,864] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Please install pyav to use video processing functions.
Please install pyav to use video processing functions.
Please install pyav to use video processing functions.
/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
OpenCLIP not installedOpenCLIP not installed

OpenCLIP not installed
[2025-08-25 19:39:50,384] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-08-25 19:39:50,390] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-08-25 19:39:50,526] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-08-25 19:39:50,526] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llava_llada to instantiate a model of type llada. This is not supported for all configurations of models and can yield errors.
Rank 0:  Overwriting config with {'use_pos_skipping': False, 'pos_skipping_range': 4096, 'mm_spatial_pool_mode': 'bilinear'}
You are using a model of type llava_llada to instantiate a model of type llada. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_llada to instantiate a model of type llada. This is not supported for all configurations of models and can yield errors.
Rank 0:  Loading vision tower: google/siglip2-so400m-patch14-384
[2025-08-25 19:40:01,882] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 1035, num_elems = 16.48B
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  6.12it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  5.16it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  6.93it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  6.32it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.01it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:03,  1.01s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:16,  3.34s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:05<00:03,  1.97s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:05<00:03,  1.96s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:06<00:13,  3.42s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:09<00:02,  2.51s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:09<00:02,  2.52s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:10<00:10,  3.46s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:13<00:06,  3.40s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:16<00:03,  3.33s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:20<00:00,  5.57s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:20<00:00,  3.46s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:20<00:00,  5.58s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:20<00:00,  3.47s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Traceback (most recent call last):
  File "/hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/train/llava/train/train_mem.py", line 4, in <module>
    train()
  File "/hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/train/llava/train/train.py", line 1891, in train
    model.get_model().initialize_vision_modules(model_args=model_args, fsdp=training_args.fsdp)
  File "/hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/train/llava/model/llava_arch.py", line 121, in initialize_vision_modules
    incompatible_keys = self.mm_projector.load_state_dict(get_w(mm_projector_weights, "mm_projector"))
  File "/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for Sequential:
	size mismatch for 0.weight: copying a param with shape torch.Size([4096, 1152]) from checkpoint, the shape in current model is torch.Size([0]).
	size mismatch for 0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).
	size mismatch for 2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([0]).
	size mismatch for 2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).
Traceback (most recent call last):
  File "/hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/train/llava/train/train_mem.py", line 4, in <module>
    train()
  File "/hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/train/llava/train/train.py", line 1891, in train
    model.get_model().initialize_vision_modules(model_args=model_args, fsdp=training_args.fsdp)
  File "/hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/train/llava/model/llava_arch.py", line 121, in initialize_vision_modules
    incompatible_keys = self.mm_projector.load_state_dict(get_w(mm_projector_weights, "mm_projector"))
  File "/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for Sequential:
	size mismatch for 0.weight: copying a param with shape torch.Size([4096, 1152]) from checkpoint, the shape in current model is torch.Size([0]).
	size mismatch for 0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).
	size mismatch for 2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([0]).
	size mismatch for 2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).
Loading checkpoint shards: 100%|██████████| 6/6 [00:21<00:00,  3.87s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:21<00:00,  3.63s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Rank 0:  Prompt version: llava_llada
Rank 0:  google/siglip2-so400m-patch14-384 is already loaded, `load_model` called again, skipping.
Traceback (most recent call last):
  File "/hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/train/llava/train/train_mem.py", line 4, in <module>
    train()
  File "/hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/train/llava/train/train.py", line 1891, in train
    model.get_model().initialize_vision_modules(model_args=model_args, fsdp=training_args.fsdp)
  File "/hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/train/llava/model/llava_arch.py", line 121, in initialize_vision_modules
    incompatible_keys = self.mm_projector.load_state_dict(get_w(mm_projector_weights, "mm_projector"))
  File "/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for Sequential:
	size mismatch for 0.weight: copying a param with shape torch.Size([4096, 1152]) from checkpoint, the shape in current model is torch.Size([0]).
	size mismatch for 0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).
	size mismatch for 2.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([0]).
	size mismatch for 2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([0]).
[2025-08-25 19:40:26,321] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1896684) of binary: /hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/bin/python
Traceback (most recent call last):
  File "/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.2', 'console_scripts', 'torchrun')())
  File "/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
llava/train/train_mem.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-08-25_19:40:26
  host      : 956aa2bad2ae
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1896685)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-08-25_19:40:26
  host      : 956aa2bad2ae
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1896686)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-25_19:40:26
  host      : 956aa2bad2ae
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1896684)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
