[2025-08-25 20:46:41,030] torch.distributed.run: [WARNING] 
[2025-08-25 20:46:41,030] torch.distributed.run: [WARNING] *****************************************
[2025-08-25 20:46:41,030] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-08-25 20:46:41,030] torch.distributed.run: [WARNING] *****************************************
[2025-08-25 20:46:45,252] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 20:46:45,259] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-25 20:46:45,260] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Please install pyav to use video processing functions.
Please install pyav to use video processing functions.
Please install pyav to use video processing functions.
/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
OpenCLIP not installedOpenCLIP not installed

OpenCLIP not installed
[2025-08-25 20:46:52,629] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-08-25 20:46:52,629] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-08-25 20:46:52,634] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-08-25 20:46:52,634] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava_llada to instantiate a model of type llada. This is not supported for all configurations of models and can yield errors.
Rank 0:  Overwriting config with {'use_pos_skipping': False, 'pos_skipping_range': 4096, 'mm_spatial_pool_mode': 'bilinear'}
You are using a model of type llava_llada to instantiate a model of type llada. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_llada to instantiate a model of type llada. This is not supported for all configurations of models and can yield errors.
Rank 0:  Loading vision tower: google/siglip2-so400m-patch14-384
[2025-08-25 20:47:01,686] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 1035, num_elems = 16.48B
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  6.00it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  5.32it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  6.80it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  6.46it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:03,  1.11s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:03,  1.10s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:18,  3.72s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:06<00:04,  2.09s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:06<00:04,  2.09s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:07<00:14,  3.70s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:09<00:02,  2.68s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:09<00:02,  2.68s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:10<00:10,  3.63s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:14<00:07,  3.64s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:18<00:03,  3.61s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:20<00:00,  5.47s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:20<00:00,  3.46s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:20<00:00,  5.48s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:20<00:00,  3.47s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:22<00:00,  3.74s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:22<00:00,  3.69s/it]
Rank 0:  Adding LoRA adapters...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Rank 0:  Prompt version: llava_llada
Rank 0:  google/siglip2-so400m-patch14-384 is already loaded, `load_model` called again, skipping.
Rank 0:  Total parameters: ~8613.17 MB)
Rank 0:  Trainable parameters: ~189.28 MB)
Rank 0:  Loading data using traditional JSON format
Rank 0:  Loading /hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/dataset/textvqa_bbox_coords_384/textvqa_bbox_coords_llava_384.json
Rank 0:  Loaded 4370 samples from /hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/dataset/textvqa_bbox_coords_384/textvqa_bbox_coords_llava_384.json
Rank 0:  Loaded 4370 samples from /hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/dataset/textvqa_bbox_coords_384/textvqa_bbox_coords_llava_384.json
Rank 0:  Formatting inputs...Skip in lazy mode
Rank 0:  Setting NCCL timeout to INF to avoid running errors.
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Parameter Offload: Total persistent parameters: 663456 in 331 params
  0%|          | 0/450 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/450 [01:10<8:46:13, 70.32s/it]                                                 {'loss': 2.2054, 'grad_norm': 4.903140722261657, 'learning_rate': 0.00019999756307053948, 'epoch': 0.02}
  0%|          | 1/450 [01:10<8:46:13, 70.32s/it]  0%|          | 2/450 [02:03<7:30:17, 60.31s/it]                                                 {'loss': 1.6213, 'grad_norm': 1.6882524127827685, 'learning_rate': 0.00019999025240093044, 'epoch': 0.04}
  0%|          | 2/450 [02:03<7:30:17, 60.31s/it]  1%|          | 3/450 [02:57<7:06:00, 57.18s/it]                                                 {'loss': 1.5096, 'grad_norm': 2.240233109021994, 'learning_rate': 0.00019997806834748456, 'epoch': 0.07}
  1%|          | 3/450 [02:57<7:06:00, 57.18s/it]