[2025-09-15 13:24:26,088] torch.distributed.run: [WARNING] 
[2025-09-15 13:24:26,088] torch.distributed.run: [WARNING] *****************************************
[2025-09-15 13:24:26,088] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-09-15 13:24:26,088] torch.distributed.run: [WARNING] *****************************************
[2025-09-15 13:24:29,735] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-15 13:24:29,745] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Please install pyav to use video processing functions.
Please install pyav to use video processing functions.
/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/hpc2hdd/home/yuxuanzhao/miniconda3/envs/dllm/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
OpenCLIP not installed
OpenCLIP not installed
[2025-09-15 13:24:34,249] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-09-15 13:24:34,289] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-09-15 13:24:34,289] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Rank 0:  Overwriting config with {'use_pos_skipping': False, 'pos_skipping_range': 4096, 'mm_spatial_pool_mode': 'bilinear'}
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:08,  1.79s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:09,  1.95s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:08,  2.07s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:08,  2.17s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:06<00:06,  2.16s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:06<00:06,  2.26s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:08<00:04,  2.19s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:08<00:04,  2.29s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:10<00:02,  2.20s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:11<00:02,  2.26s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:12<00:00,  1.98s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:12<00:00,  2.05s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:12<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:12<00:00,  2.13s/it]
Rank 0:  Adding LoRA adapters...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Rank 0:  Prompt version: llava_llada
Rank 0:  Loading vision tower: /hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/train/models/siglip2-so400m-patch14-384
Rank 0:  Total parameters: ~8518.72 MB)
Rank 0:  Trainable parameters: ~105.39 MB)
Rank 0:  Loading data using traditional JSON format
Rank 0:  Loading /hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/dataset/coco2017/llava_multi/coco_val2017_grouped_by_category.json
Rank 0:  Loaded 4952 samples from /hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/dataset/coco2017/llava_multi/coco_val2017_grouped_by_category.json
Rank 0:  Loaded 4952 samples from /hpc2ssd/JH_DATA/spooler/yuxuanzhao/lijungang/wujie/LLaDA-V/dataset/coco2017/llava_multi/coco_val2017_grouped_by_category.json
Rank 0:  Formatting inputs...Skip in lazy mode
Rank 0:  Setting NCCL timeout to INF to avoid running errors.
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using /hpc2hdd/home/yuxuanzhao/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /hpc2hdd/home/yuxuanzhao/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /hpc2hdd/home/yuxuanzhao/.cache/torch_extensions/py39_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.5797677040100098 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.6224219799041748 seconds
  0%|          | 0/154 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)
=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  1%|          | 1/154 [00:13<33:37, 13.19s/it]                                               {'loss': 0.9378, 'grad_norm': 11.679586250606294, 'learning_rate': 2e-05, 'epoch': 0.01}
  1%|          | 1/154 [00:13<33:37, 13.19s/it]  1%|▏         | 2/154 [00:19<23:49,  9.41s/it]                                               {'loss': 1.4901, 'grad_norm': 10.120966732004016, 'learning_rate': 4e-05, 'epoch': 0.01}
  1%|▏         | 2/154 [00:19<23:49,  9.41s/it]  2%|▏         | 3/154 [00:29<24:09,  9.60s/it]                                               {'loss': 0.6913, 'grad_norm': 3.356072936546511, 'learning_rate': 6e-05, 'epoch': 0.02}
  2%|▏         | 3/154 [00:29<24:09,  9.60s/it]  3%|▎         | 4/154 [00:39<23:42,  9.49s/it]                                               {'loss': 0.7106, 'grad_norm': 2.118834133221958, 'learning_rate': 8e-05, 'epoch': 0.03}
  3%|▎         | 4/154 [00:39<23:42,  9.49s/it]  3%|▎         | 5/154 [00:47<22:48,  9.19s/it]                                               {'loss': 0.8075, 'grad_norm': 1.6858234554321314, 'learning_rate': 0.0001, 'epoch': 0.03}
  3%|▎         | 5/154 [00:47<22:48,  9.19s/it]  4%|▍         | 6/154 [00:56<22:27,  9.10s/it]                                               {'loss': 0.7443, 'grad_norm': 1.713574332840902, 'learning_rate': 9.99888864929809e-05, 'epoch': 0.04}
  4%|▍         | 6/154 [00:56<22:27,  9.10s/it]  5%|▍         | 7/154 [01:04<21:28,  8.77s/it]                                               {'loss': 0.7312, 'grad_norm': 1.6739647185277629, 'learning_rate': 9.995555091232516e-05, 'epoch': 0.05}
  5%|▍         | 7/154 [01:04<21:28,  8.77s/it]  5%|▌         | 8/154 [01:13<21:24,  8.79s/it]                                               {'loss': 0.5609, 'grad_norm': 1.0320214940316754, 'learning_rate': 9.990000807704114e-05, 'epoch': 0.05}
  5%|▌         | 8/154 [01:13<21:24,  8.79s/it]  6%|▌         | 9/154 [01:20<19:45,  8.18s/it]                                               {'loss': 0.6676, 'grad_norm': 2.3946744244385414, 'learning_rate': 9.982228267815643e-05, 'epoch': 0.06}
  6%|▌         | 9/154 [01:20<19:45,  8.18s/it]  6%|▋         | 10/154 [01:28<19:21,  8.06s/it]                                                {'loss': 0.7875, 'grad_norm': 1.148566517751616, 'learning_rate': 9.972240926774168e-05, 'epoch': 0.06}
  6%|▋         | 10/154 [01:28<19:21,  8.06s/it]  7%|▋         | 11/154 [01:36<19:41,  8.26s/it]                                                {'loss': 0.4649, 'grad_norm': 0.4311491641038615, 'learning_rate': 9.96004322435508e-05, 'epoch': 0.07}
  7%|▋         | 11/154 [01:36<19:41,  8.26s/it]  8%|▊         | 12/154 [01:44<19:15,  8.14s/it]                                                {'loss': 0.6356, 'grad_norm': 0.6512753308910706, 'learning_rate': 9.945640582928437e-05, 'epoch': 0.08}
  8%|▊         | 12/154 [01:44<19:15,  8.14s/it]  8%|▊         | 13/154 [01:52<18:59,  8.09s/it]                                                {'loss': 0.5737, 'grad_norm': 0.4546199342006478, 'learning_rate': 9.929039405048501e-05, 'epoch': 0.08}
  8%|▊         | 13/154 [01:52<18:59,  8.09s/it]  9%|▉         | 14/154 [02:01<19:20,  8.29s/it]                                                {'loss': 0.6323, 'grad_norm': 4.920957255707471, 'learning_rate': 9.910247070607552e-05, 'epoch': 0.09}
  9%|▉         | 14/154 [02:01<19:20,  8.29s/it] 10%|▉         | 15/154 [02:09<19:03,  8.23s/it]                                                {'loss': 0.499, 'grad_norm': 0.9938413398079408, 'learning_rate': 9.889271933555213e-05, 'epoch': 0.1}
 10%|▉         | 15/154 [02:09<19:03,  8.23s/it] 10%|█         | 16/154 [02:17<18:27,  8.03s/it]                                                {'loss': 0.4736, 'grad_norm': 0.544726046031599, 'learning_rate': 9.866123318184803e-05, 'epoch': 0.1}
 10%|█         | 16/154 [02:17<18:27,  8.03s/it] 11%|█         | 17/154 [02:25<18:22,  8.05s/it]                                                {'loss': 0.4834, 'grad_norm': 0.48452425779767344, 'learning_rate': 9.840811514988294e-05, 'epoch': 0.11}
 11%|█         | 17/154 [02:25<18:22,  8.05s/it] 12%|█▏        | 18/154 [02:33<18:14,  8.05s/it]                                                {'loss': 0.4959, 'grad_norm': 0.36138367097071755, 'learning_rate': 9.813347776081789e-05, 'epoch': 0.12}
 12%|█▏        | 18/154 [02:33<18:14,  8.05s/it] 12%|█▏        | 19/154 [02:41<18:06,  8.05s/it]                                                {'loss': 0.4969, 'grad_norm': 0.24263162806093747, 'learning_rate': 9.783744310203491e-05, 'epoch': 0.12}
 12%|█▏        | 19/154 [02:41<18:06,  8.05s/it] 13%|█▎        | 20/154 [02:50<18:51,  8.44s/it]                                                {'loss': 0.4963, 'grad_norm': 0.21527879190889715, 'learning_rate': 9.752014277286432e-05, 'epoch': 0.13}
 13%|█▎        | 20/154 [02:50<18:51,  8.44s/it] 14%|█▎        | 21/154 [03:00<19:36,  8.85s/it]                                                {'loss': 0.4439, 'grad_norm': 0.28266003592988576, 'learning_rate': 9.718171782608356e-05, 'epoch': 0.14}
 14%|█▎        | 21/154 [03:00<19:36,  8.85s/it] 14%|█▍        | 22/154 [03:08<19:06,  8.69s/it]                                                {'loss': 0.4728, 'grad_norm': 1.0873253824639908, 'learning_rate': 9.682231870521347e-05, 'epoch': 0.14}
 14%|█▍        | 22/154 [03:08<19:06,  8.69s/it] 15%|█▍        | 23/154 [03:16<18:31,  8.49s/it]                                                {'loss': 0.4621, 'grad_norm': 0.4342083741740077, 'learning_rate': 9.644210517764014e-05, 'epoch': 0.15}
 15%|█▍        | 23/154 [03:16<18:31,  8.49s/it] 16%|█▌        | 24/154 [03:26<18:51,  8.71s/it]                                                {'loss': 0.4198, 'grad_norm': 0.18890070257884198, 'learning_rate': 9.60412462635919e-05, 'epoch': 0.16}
 16%|█▌        | 24/154 [03:26<18:51,  8.71s/it] 16%|█▌        | 25/154 [03:34<18:17,  8.51s/it]                                                {'loss': 0.4427, 'grad_norm': 0.5125429926261417, 'learning_rate': 9.561992016100293e-05, 'epoch': 0.16}
 16%|█▌        | 25/154 [03:34<18:17,  8.51s/it] 17%|█▋        | 26/154 [03:41<17:29,  8.20s/it]                                                {'loss': 0.4801, 'grad_norm': 0.4373317292726038, 'learning_rate': 9.517831416629716e-05, 'epoch': 0.17}
 17%|█▋        | 26/154 [03:41<17:29,  8.20s/it] 18%|█▊        | 27/154 [03:49<17:06,  8.09s/it]                                                {'loss': 0.5487, 'grad_norm': 1.1403680799450537, 'learning_rate': 9.471662459112747e-05, 'epoch': 0.17}
 18%|█▊        | 27/154 [03:49<17:06,  8.09s/it] 18%|█▊        | 28/154 [03:57<17:06,  8.15s/it]                                                {'loss': 0.4484, 'grad_norm': 0.4893800502823438, 'learning_rate': 9.423505667510724e-05, 'epoch': 0.18}
 18%|█▊        | 28/154 [03:57<17:06,  8.15s/it] 19%|█▉        | 29/154 [04:06<17:21,  8.34s/it]                                                {'loss': 0.4071, 'grad_norm': 0.16544486835277994, 'learning_rate': 9.373382449457304e-05, 'epoch': 0.19}
 19%|█▉        | 29/154 [04:06<17:21,  8.34s/it] 19%|█▉        | 30/154 [04:14<16:51,  8.16s/it]                                                {'loss': 0.4125, 'grad_norm': 0.3021811321498879, 'learning_rate': 9.321315086741916e-05, 'epoch': 0.19}
 19%|█▉        | 30/154 [04:14<16:51,  8.16s/it] 20%|██        | 31/154 [04:23<17:23,  8.48s/it]                                                {'loss': 0.4776, 'grad_norm': 0.6895643798282489, 'learning_rate': 9.267326725404599e-05, 'epoch': 0.2}
 20%|██        | 31/154 [04:23<17:23,  8.48s/it] 21%|██        | 32/154 [04:31<17:12,  8.46s/it]                                                {'loss': 0.4651, 'grad_norm': 0.23052161628726933, 'learning_rate': 9.21144136544666e-05, 'epoch': 0.21}
 21%|██        | 32/154 [04:31<17:12,  8.46s/it] 21%|██▏       | 33/154 [04:39<16:45,  8.31s/it]                                                {'loss': 0.4563, 'grad_norm': 0.7438593455484563, 'learning_rate': 9.153683850161706e-05, 'epoch': 0.21}
 21%|██▏       | 33/154 [04:39<16:45,  8.31s/it] 22%|██▏       | 34/154 [04:47<16:19,  8.16s/it]                                                {'loss': 0.4137, 'grad_norm': 0.23583325678944328, 'learning_rate': 9.094079855091797e-05, 'epoch': 0.22}
 22%|██▏       | 34/154 [04:47<16:19,  8.16s/it] 23%|██▎       | 35/154 [04:55<16:05,  8.11s/it]                                                {'loss': 0.412, 'grad_norm': 0.18530818242644756, 'learning_rate': 9.032655876613636e-05, 'epoch': 0.23}
 23%|██▎       | 35/154 [04:55<16:05,  8.11s/it] 23%|██▎       | 36/154 [05:03<15:51,  8.07s/it]                                                {'loss': 0.484, 'grad_norm': 0.5569160181730815, 'learning_rate': 8.96943922015986e-05, 'epoch': 0.23}
 23%|██▎       | 36/154 [05:03<15:51,  8.07s/it] 24%|██▍       | 37/154 [05:11<15:49,  8.11s/it]                                                {'loss': 0.4843, 'grad_norm': 0.6672592410394197, 'learning_rate': 8.904457988080681e-05, 'epoch': 0.24}
 24%|██▍       | 37/154 [05:11<15:49,  8.11s/it] 25%|██▍       | 38/154 [05:19<15:41,  8.11s/it]                                                {'loss': 0.5097, 'grad_norm': 0.8442701926082853, 'learning_rate': 8.83774106715125e-05, 'epoch': 0.25}
 25%|██▍       | 38/154 [05:19<15:41,  8.11s/it] 25%|██▌       | 39/154 [05:27<15:27,  8.06s/it]                                                {'loss': 0.4899, 'grad_norm': 0.3246479869881172, 'learning_rate': 8.76931811573033e-05, 'epoch': 0.25}
 25%|██▌       | 39/154 [05:27<15:27,  8.06s/it] 26%|██▌       | 40/154 [05:35<15:16,  8.04s/it]                                                {'loss': 0.4981, 'grad_norm': 1.9818382569187916, 'learning_rate': 8.699219550575953e-05, 'epoch': 0.26}
 26%|██▌       | 40/154 [05:35<15:16,  8.04s/it] 27%|██▋       | 41/154 [05:44<15:37,  8.30s/it]                                                {'loss': 0.4495, 'grad_norm': 0.6496014088172622, 'learning_rate': 8.627476533323957e-05, 'epoch': 0.26}
 27%|██▋       | 41/154 [05:44<15:37,  8.30s/it] 27%|██▋       | 42/154 [05:53<15:35,  8.35s/it]                                                {'loss': 0.4086, 'grad_norm': 0.27124822761026907, 'learning_rate': 8.554120956635375e-05, 'epoch': 0.27}
 27%|██▋       | 42/154 [05:53<15:35,  8.35s/it] 28%|██▊       | 43/154 [06:01<15:25,  8.33s/it]                                                {'loss': 0.486, 'grad_norm': 1.0690791688370571, 'learning_rate': 8.479185430018858e-05, 'epoch': 0.28}
 28%|██▊       | 43/154 [06:01<15:25,  8.33s/it] 29%|██▊       | 44/154 [06:10<15:34,  8.49s/it]                                                {'loss': 0.565, 'grad_norm': 0.911514316837706, 'learning_rate': 8.402703265334455e-05, 'epoch': 0.28}
 29%|██▊       | 44/154 [06:10<15:34,  8.49s/it] 29%|██▉       | 45/154 [06:18<15:24,  8.49s/it]                                                {'loss': 0.3874, 'grad_norm': 0.25095011055923583, 'learning_rate': 8.324708461985124e-05, 'epoch': 0.29}
 29%|██▉       | 45/154 [06:18<15:24,  8.49s/it] 30%|██▉       | 46/154 [06:26<14:39,  8.14s/it]                                                {'loss': 0.5164, 'grad_norm': 0.3778126064581204, 'learning_rate': 8.245235691802644e-05, 'epoch': 0.3}
 30%|██▉       | 46/154 [06:26<14:39,  8.14s/it] 31%|███       | 47/154 [06:34<14:25,  8.09s/it]                                                {'loss': 0.5502, 'grad_norm': 0.6430471268228041, 'learning_rate': 8.164320283634585e-05, 'epoch': 0.3}
 31%|███       | 47/154 [06:34<14:25,  8.09s/it] 31%|███       | 48/154 [06:42<14:13,  8.05s/it]                                                {'loss': 0.5124, 'grad_norm': 0.7818229100525933, 'learning_rate': 8.081998207639212e-05, 'epoch': 0.31}
 31%|███       | 48/154 [06:42<14:13,  8.05s/it] 32%|███▏      | 49/154 [06:50<14:17,  8.17s/it]                                                {'loss': 0.4755, 'grad_norm': 1.4208448262276552, 'learning_rate': 7.998306059295301e-05, 'epoch': 0.32}
 32%|███▏      | 49/154 [06:50<14:17,  8.17s/it] 32%|███▏      | 50/154 [06:58<13:49,  7.98s/it]                                                {'loss': 0.4728, 'grad_norm': 0.1937429399896069, 'learning_rate': 7.913281043133978e-05, 'epoch': 0.32}
 32%|███▏      | 50/154 [06:58<13:49,  7.98s/it] 33%|███▎      | 51/154 [07:06<14:05,  8.21s/it]                                                {'loss': 0.3761, 'grad_norm': 0.11164658156161615, 'learning_rate': 7.826960956199794e-05, 'epoch': 0.33}
 33%|███▎      | 51/154 [07:06<14:05,  8.21s/it] 34%|███▍      | 52/154 [07:15<14:14,  8.38s/it]                                                {'loss': 0.4264, 'grad_norm': 0.34617179954973937, 'learning_rate': 7.739384171248435e-05, 'epoch': 0.34}
 34%|███▍      | 52/154 [07:15<14:14,  8.38s/it] 34%|███▍      | 53/154 [07:24<14:11,  8.44s/it]                                                {'loss': 0.3907, 'grad_norm': 0.12451957977013471, 'learning_rate': 7.650589619688469e-05, 'epoch': 0.34}
 34%|███▍      | 53/154 [07:24<14:11,  8.44s/it] 35%|███▌      | 54/154 [07:33<14:18,  8.59s/it]                                                {'loss': 0.3801, 'grad_norm': 0.18700369078135473, 'learning_rate': 7.560616774274775e-05, 'epoch': 0.35}
 35%|███▌      | 54/154 [07:33<14:18,  8.59s/it] 36%|███▌      | 55/154 [07:41<14:16,  8.65s/it]                                                {'loss': 0.3916, 'grad_norm': 0.33374167430151264, 'learning_rate': 7.469505631561317e-05, 'epoch': 0.36}
 36%|███▌      | 55/154 [07:41<14:16,  8.65s/it] 36%|███▋      | 56/154 [07:50<14:02,  8.60s/it]                                                {'loss': 0.5093, 'grad_norm': 0.3128698782644505, 'learning_rate': 7.377296694121058e-05, 'epoch': 0.36}
 36%|███▋      | 56/154 [07:50<14:02,  8.60s/it] 37%|███▋      | 57/154 [07:58<13:28,  8.34s/it]                                                {'loss': 0.519, 'grad_norm': 0.4053611752319709, 'learning_rate': 7.284030952540937e-05, 'epoch': 0.37}
 37%|███▋      | 57/154 [07:58<13:28,  8.34s/it] 38%|███▊      | 58/154 [08:05<12:57,  8.10s/it]                                                {'loss': 0.4845, 'grad_norm': 0.1226798973249322, 'learning_rate': 7.189749867199899e-05, 'epoch': 0.37}
 38%|███▊      | 58/154 [08:05<12:57,  8.10s/it] 38%|███▊      | 59/154 [08:13<12:31,  7.91s/it]                                                {'loss': 0.4689, 'grad_norm': 0.20121834968092225, 'learning_rate': 7.094495349838092e-05, 'epoch': 0.38}
 38%|███▊      | 59/154 [08:13<12:31,  7.91s/it] 39%|███▉      | 60/154 [08:21<12:34,  8.03s/it]                                                {'loss': 0.4173, 'grad_norm': 0.26657132267186895, 'learning_rate': 6.998309744925411e-05, 'epoch': 0.39}
 39%|███▉      | 60/154 [08:21<12:34,  8.03s/it] 40%|███▉      | 61/154 [08:30<12:51,  8.30s/it]                                                {'loss': 0.5101, 'grad_norm': 1.412739973022866, 'learning_rate': 6.901235810837669e-05, 'epoch': 0.39}
 40%|███▉      | 61/154 [08:30<12:51,  8.30s/it] 40%|████      | 62/154 [08:38<12:51,  8.38s/it]                                                {'loss': 0.478, 'grad_norm': 0.36892059873460586, 'learning_rate': 6.803316700848779e-05, 'epoch': 0.4}
 40%|████      | 62/154 [08:38<12:51,  8.38s/it] 41%|████      | 63/154 [08:47<12:56,  8.53s/it]                                                {'loss': 0.4569, 'grad_norm': 0.4228328498065205, 'learning_rate': 6.704595943947385e-05, 'epoch': 0.41}
 41%|████      | 63/154 [08:47<12:56,  8.53s/it] 42%|████▏     | 64/154 [08:56<12:59,  8.66s/it]                                                {'loss': 0.4981, 'grad_norm': 0.07946317633063786, 'learning_rate': 6.605117425486482e-05, 'epoch': 0.41}
 42%|████▏     | 64/154 [08:56<12:59,  8.66s/it] 42%|████▏     | 65/154 [09:05<12:38,  8.52s/it]                                                {'loss': 0.4164, 'grad_norm': 0.13321393715837695, 'learning_rate': 6.504925367674594e-05, 'epoch': 0.42}
 42%|████▏     | 65/154 [09:05<12:38,  8.52s/it] 43%|████▎     | 66/154 [09:13<12:33,  8.56s/it]                                                {'loss': 0.4293, 'grad_norm': 0.2223512920218361, 'learning_rate': 6.404064309917231e-05, 'epoch': 0.43}
 43%|████▎     | 66/154 [09:13<12:33,  8.56s/it] 44%|████▎     | 67/154 [09:22<12:38,  8.72s/it]                                                {'loss': 0.416, 'grad_norm': 0.6525177409511652, 'learning_rate': 6.302579089017327e-05, 'epoch': 0.43}
 44%|████▎     | 67/154 [09:22<12:38,  8.72s/it] 44%|████▍     | 68/154 [09:31<12:25,  8.67s/it]                                                {'loss': 0.4991, 'grad_norm': 0.1796745834683863, 'learning_rate': 6.200514819243476e-05, 'epoch': 0.44}
 44%|████▍     | 68/154 [09:31<12:25,  8.67s/it] 45%|████▍     | 69/154 [09:39<11:56,  8.43s/it]                                                {'loss': 0.4986, 'grad_norm': 0.3469533565445737, 'learning_rate': 6.097916872274815e-05, 'epoch': 0.45}
 45%|████▍     | 69/154 [09:39<11:56,  8.43s/it] 45%|████▌     | 70/154 [09:46<11:31,  8.23s/it]                                                {'loss': 0.4796, 'grad_norm': 0.2989177569434243, 'learning_rate': 5.994830857031499e-05, 'epoch': 0.45}
 45%|████▌     | 70/154 [09:46<11:31,  8.23s/it] 46%|████▌     | 71/154 [09:55<11:22,  8.22s/it]                                                {'loss': 0.3844, 'grad_norm': 0.10635082180611821, 'learning_rate': 5.891302599399685e-05, 'epoch': 0.46}
 46%|████▌     | 71/154 [09:55<11:22,  8.22s/it] 47%|████▋     | 72/154 [10:03<11:22,  8.32s/it]                                                {'loss': 0.3854, 'grad_norm': 0.11381406690046827, 'learning_rate': 5.78737812186009e-05, 'epoch': 0.47}
 47%|████▋     | 72/154 [10:03<11:22,  8.32s/it] 47%|████▋     | 73/154 [10:10<10:32,  7.80s/it]                                                {'loss': 0.522, 'grad_norm': 0.2610177858840385, 'learning_rate': 5.683103623029135e-05, 'epoch': 0.47}
 47%|████▋     | 73/154 [10:10<10:32,  7.80s/it] 48%|████▊     | 74/154 [10:18<10:36,  7.95s/it]                                                {'loss': 0.416, 'grad_norm': 0.11555036765204738, 'learning_rate': 5.578525457121807e-05, 'epoch': 0.48}
 48%|████▊     | 74/154 [10:18<10:36,  7.95s/it] 49%|████▊     | 75/154 [10:26<10:29,  7.97s/it]                                                {'loss': 0.4115, 'grad_norm': 0.25263833604670227, 'learning_rate': 5.473690113345342e-05, 'epoch': 0.48}
 49%|████▊     | 75/154 [10:26<10:29,  7.97s/it] 49%|████▉     | 76/154 [10:34<10:29,  8.07s/it]                                                {'loss': 0.4777, 'grad_norm': 0.14427784020201756, 'learning_rate': 5.368644195232896e-05, 'epoch': 0.49}
 49%|████▉     | 76/154 [10:34<10:29,  8.07s/it] 50%|█████     | 77/154 [10:42<10:20,  8.06s/it]                                                {'loss': 0.4164, 'grad_norm': 0.14418964353678654, 'learning_rate': 5.263434399926398e-05, 'epoch': 0.5}
 50%|█████     | 77/154 [10:42<10:20,  8.06s/it] 51%|█████     | 78/154 [10:52<10:44,  8.49s/it]                                                {'loss': 0.511, 'grad_norm': 3.865669337814771, 'learning_rate': 5.158107497417795e-05, 'epoch': 0.5}
 51%|█████     | 78/154 [10:52<10:44,  8.49s/it] 51%|█████▏    | 79/154 [11:00<10:36,  8.48s/it]                                                {'loss': 0.4823, 'grad_norm': 0.4130194319628705, 'learning_rate': 5.052710309757899e-05, 'epoch': 0.51}
 51%|█████▏    | 79/154 [11:00<10:36,  8.48s/it] 52%|█████▏    | 80/154 [11:09<10:37,  8.62s/it]                                                {'loss': 0.4686, 'grad_norm': 0.2718588489909093, 'learning_rate': 4.947289690242102e-05, 'epoch': 0.52}
 52%|█████▏    | 80/154 [11:09<10:37,  8.62s/it] 53%|█████▎    | 81/154 [11:16<09:55,  8.15s/it]                                                {'loss': 0.5766, 'grad_norm': 1.0646778674371522, 'learning_rate': 4.841892502582206e-05, 'epoch': 0.52}
 53%|█████▎    | 81/154 [11:16<09:55,  8.15s/it] 53%|█████▎    | 82/154 [11:25<09:58,  8.31s/it]                                                {'loss': 0.4588, 'grad_norm': 0.2289506081575746, 'learning_rate': 4.736565600073602e-05, 'epoch': 0.53}
 53%|█████▎    | 82/154 [11:25<09:58,  8.31s/it] 54%|█████▍    | 83/154 [11:33<09:47,  8.27s/it]                                                {'loss': 0.4374, 'grad_norm': 0.12055647777795125, 'learning_rate': 4.631355804767105e-05, 'epoch': 0.54}
 54%|█████▍    | 83/154 [11:33<09:47,  8.27s/it] 55%|█████▍    | 84/154 [11:42<09:41,  8.31s/it]                                                {'loss': 0.4574, 'grad_norm': 0.09894163112064362, 'learning_rate': 4.5263098866546586e-05, 'epoch': 0.54}
 55%|█████▍    | 84/154 [11:42<09:41,  8.31s/it] 55%|█████▌    | 85/154 [11:49<09:17,  8.08s/it]                                                {'loss': 0.5014, 'grad_norm': 0.5179863813112343, 'learning_rate': 4.421474542878195e-05, 'epoch': 0.55}
 55%|█████▌    | 85/154 [11:49<09:17,  8.08s/it] 56%|█████▌    | 86/154 [11:58<09:26,  8.33s/it]                                                {'loss': 0.4843, 'grad_norm': 0.1330571865805528, 'learning_rate': 4.316896376970866e-05, 'epoch': 0.56}
 56%|█████▌    | 86/154 [11:58<09:26,  8.33s/it] 56%|█████▋    | 87/154 [12:06<09:08,  8.19s/it]                                                {'loss': 0.4298, 'grad_norm': 0.25207393913787335, 'learning_rate': 4.212621878139912e-05, 'epoch': 0.56}
 56%|█████▋    | 87/154 [12:06<09:08,  8.19s/it] 57%|█████▋    | 88/154 [12:13<08:41,  7.90s/it]                                                {'loss': 0.52, 'grad_norm': 0.23940369965589467, 'learning_rate': 4.108697400600316e-05, 'epoch': 0.57}
 57%|█████▋    | 88/154 [12:13<08:41,  7.90s/it] 58%|█████▊    | 89/154 [12:21<08:28,  7.82s/it]                                                {'loss': 0.4235, 'grad_norm': 0.299008122007709, 'learning_rate': 4.005169142968503e-05, 'epoch': 0.58}
 58%|█████▊    | 89/154 [12:21<08:28,  7.82s/it] 58%|█████▊    | 90/154 [12:31<09:04,  8.51s/it]                                                {'loss': 0.3866, 'grad_norm': 0.10776760645512272, 'learning_rate': 3.9020831277251863e-05, 'epoch': 0.58}
 58%|█████▊    | 90/154 [12:31<09:04,  8.51s/it] 59%|█████▉    | 91/154 [12:41<09:20,  8.89s/it]                                                {'loss': 0.3918, 'grad_norm': 0.529121482476957, 'learning_rate': 3.7994851807565254e-05, 'epoch': 0.59}
 59%|█████▉    | 91/154 [12:41<09:20,  8.89s/it] 60%|█████▉    | 92/154 [12:49<08:59,  8.70s/it]                                                {'loss': 0.4312, 'grad_norm': 0.19307380357231088, 'learning_rate': 3.6974209109826726e-05, 'epoch': 0.59}
 60%|█████▉    | 92/154 [12:49<08:59,  8.70s/it] 60%|██████    | 93/154 [12:57<08:30,  8.38s/it]                                                {'loss': 0.4176, 'grad_norm': 0.6505986473836949, 'learning_rate': 3.595935690082769e-05, 'epoch': 0.6}
 60%|██████    | 93/154 [12:57<08:30,  8.38s/it] 61%|██████    | 94/154 [13:06<08:42,  8.70s/it]                                                {'loss': 0.4722, 'grad_norm': 0.3154898193488138, 'learning_rate': 3.495074632325407e-05, 'epoch': 0.61}
 61%|██████    | 94/154 [13:06<08:42,  8.70s/it] 62%|██████▏   | 95/154 [13:14<08:20,  8.48s/it]                                                {'loss': 0.387, 'grad_norm': 0.4206181985041631, 'learning_rate': 3.394882574513519e-05, 'epoch': 0.61}
 62%|██████▏   | 95/154 [13:14<08:20,  8.48s/it] 62%|██████▏   | 96/154 [13:21<07:52,  8.14s/it]                                                {'loss': 0.5054, 'grad_norm': 0.4487467068628385, 'learning_rate': 3.295404056052616e-05, 'epoch': 0.62}
 62%|██████▏   | 96/154 [13:21<07:52,  8.14s/it] 63%|██████▎   | 97/154 [13:30<07:50,  8.26s/it]                                                {'loss': 0.4125, 'grad_norm': 0.11378428154804002, 'learning_rate': 3.196683299151223e-05, 'epoch': 0.63}
 63%|██████▎   | 97/154 [13:30<07:50,  8.26s/it] 64%|██████▎   | 98/154 [13:39<07:48,  8.37s/it]                                                {'loss': 0.4127, 'grad_norm': 0.23033943427678272, 'learning_rate': 3.098764189162332e-05, 'epoch': 0.63}
 64%|██████▎   | 98/154 [13:39<07:48,  8.37s/it] 64%|██████▍   | 99/154 [13:46<07:27,  8.14s/it]                                                {'loss': 0.4625, 'grad_norm': 0.16510468933151098, 'learning_rate': 3.0016902550745897e-05, 'epoch': 0.64}
 64%|██████▍   | 99/154 [13:46<07:27,  8.14s/it] 65%|██████▍   | 100/154 [13:54<07:15,  8.06s/it]                                                 {'loss': 0.4069, 'grad_norm': 0.07118343667108931, 'learning_rate': 2.905504650161909e-05, 'epoch': 0.65}
 65%|██████▍   | 100/154 [13:54<07:15,  8.06s/it] 66%|██████▌   | 101/154 [14:02<07:00,  7.94s/it]                                                 {'loss': 0.4465, 'grad_norm': 0.15545314609873678, 'learning_rate': 2.810250132800103e-05, 'epoch': 0.65}
 66%|██████▌   | 101/154 [14:02<07:00,  7.94s/it] 66%|██████▌   | 102/154 [14:11<07:14,  8.35s/it]                                                 {'loss': 0.421, 'grad_norm': 0.1682685350309388, 'learning_rate': 2.715969047459066e-05, 'epoch': 0.66}
 66%|██████▌   | 102/154 [14:11<07:14,  8.35s/it] 67%|██████▋   | 103/154 [14:19<07:00,  8.25s/it]                                                 {'loss': 0.4683, 'grad_norm': 0.2146938754888336, 'learning_rate': 2.6227033058789408e-05, 'epoch': 0.67}
 67%|██████▋   | 103/154 [14:19<07:00,  8.25s/it] 68%|██████▊   | 104/154 [14:27<06:45,  8.11s/it]                                                 {'loss': 0.4477, 'grad_norm': 0.42736734468086796, 'learning_rate': 2.530494368438683e-05, 'epoch': 0.67}
 68%|██████▊   | 104/154 [14:27<06:45,  8.11s/it] 68%|██████▊   | 105/154 [14:34<06:29,  7.95s/it]                                                 {'loss': 0.5304, 'grad_norm': 0.7486381882878341, 'learning_rate': 2.4393832257252252e-05, 'epoch': 0.68}
 68%|██████▊   | 105/154 [14:34<06:29,  7.95s/it] 69%|██████▉   | 106/154 [14:42<06:23,  8.00s/it]                                                 {'loss': 0.3962, 'grad_norm': 0.17810420450321798, 'learning_rate': 2.349410380311532e-05, 'epoch': 0.68}
 69%|██████▉   | 106/154 [14:42<06:23,  8.00s/it] 69%|██████▉   | 107/154 [14:50<06:09,  7.86s/it]                                                 {'loss': 0.4476, 'grad_norm': 0.8212340491849138, 'learning_rate': 2.260615828751566e-05, 'epoch': 0.69}
 69%|██████▉   | 107/154 [14:50<06:09,  7.86s/it] 70%|███████   | 108/154 [14:57<05:51,  7.64s/it]                                                 {'loss': 0.5023, 'grad_norm': 0.4805239824012273, 'learning_rate': 2.173039043800206e-05, 'epoch': 0.7}
 70%|███████   | 108/154 [14:57<05:51,  7.64s/it] 71%|███████   | 109/154 [15:05<05:44,  7.65s/it]                                                 {'loss': 0.4141, 'grad_norm': 0.22568765634126195, 'learning_rate': 2.086718956866024e-05, 'epoch': 0.7}
 71%|███████   | 109/154 [15:05<05:44,  7.65s/it] 71%|███████▏  | 110/154 [15:13<05:43,  7.81s/it]                                                 {'loss': 0.4453, 'grad_norm': 0.11818126679328106, 'learning_rate': 2.0016939407046987e-05, 'epoch': 0.71}
 71%|███████▏  | 110/154 [15:13<05:43,  7.81s/it] 72%|███████▏  | 111/154 [15:22<05:52,  8.19s/it]                                                 {'loss': 0.4825, 'grad_norm': 0.27668571313729556, 'learning_rate': 1.9180017923607886e-05, 'epoch': 0.72}
 72%|███████▏  | 111/154 [15:22<05:52,  8.19s/it] 73%|███████▎  | 112/154 [15:30<05:45,  8.22s/it]                                                 {'loss': 0.4056, 'grad_norm': 0.5939719638533805, 'learning_rate': 1.835679716365417e-05, 'epoch': 0.72}
 73%|███████▎  | 112/154 [15:30<05:45,  8.22s/it] 73%|███████▎  | 113/154 [15:38<05:28,  8.02s/it]                                                 {'loss': 0.4686, 'grad_norm': 0.883646107276791, 'learning_rate': 1.754764308197358e-05, 'epoch': 0.73}
 73%|███████▎  | 113/154 [15:38<05:28,  8.02s/it] 74%|███████▍  | 114/154 [15:46<05:25,  8.14s/it]                                                 {'loss': 0.3934, 'grad_norm': 0.0889593369740384, 'learning_rate': 1.675291538014877e-05, 'epoch': 0.74}
 74%|███████▍  | 114/154 [15:46<05:25,  8.14s/it] 75%|███████▍  | 115/154 [15:53<05:01,  7.74s/it]                                                 {'loss': 0.493, 'grad_norm': 0.671936875089068, 'learning_rate': 1.5972967346655448e-05, 'epoch': 0.74}
 75%|███████▍  | 115/154 [15:53<05:01,  7.74s/it] 75%|███████▌  | 116/154 [16:01<04:50,  7.66s/it]                                                 {'loss': 0.4459, 'grad_norm': 0.3578587764482981, 'learning_rate': 1.5208145699811415e-05, 'epoch': 0.75}
 75%|███████▌  | 116/154 [16:01<04:50,  7.66s/it] 76%|███████▌  | 117/154 [16:09<04:47,  7.77s/it]                                                 {'loss': 0.4636, 'grad_norm': 0.4717480943567533, 'learning_rate': 1.4458790433646263e-05, 'epoch': 0.76}
 76%|███████▌  | 117/154 [16:09<04:47,  7.77s/it] 77%|███████▋  | 118/154 [16:16<04:33,  7.59s/it]                                                 {'loss': 0.5648, 'grad_norm': 0.5687559651491677, 'learning_rate': 1.3725234666760428e-05, 'epoch': 0.76}
 77%|███████▋  | 118/154 [16:16<04:33,  7.59s/it] 77%|███████▋  | 119/154 [16:24<04:33,  7.83s/it]                                                 {'loss': 0.5383, 'grad_norm': 0.3938108609105932, 'learning_rate': 1.3007804494240478e-05, 'epoch': 0.77}
 77%|███████▋  | 119/154 [16:24<04:33,  7.83s/it] 78%|███████▊  | 120/154 [16:32<04:24,  7.77s/it]                                                 {'loss': 0.5106, 'grad_norm': 0.43382301798816025, 'learning_rate': 1.2306818842696716e-05, 'epoch': 0.78}
 78%|███████▊  | 120/154 [16:32<04:24,  7.77s/it] 79%|███████▊  | 121/154 [16:40<04:20,  7.91s/it]                                                 {'loss': 0.4044, 'grad_norm': 0.155574291194654, 'learning_rate': 1.1622589328487504e-05, 'epoch': 0.78}
 79%|███████▊  | 121/154 [16:40<04:20,  7.91s/it] 79%|███████▉  | 122/154 [16:48<04:10,  7.83s/it]                                                 {'loss': 0.4896, 'grad_norm': 0.4903715481282917, 'learning_rate': 1.0955420119193199e-05, 'epoch': 0.79}
 79%|███████▉  | 122/154 [16:48<04:10,  7.83s/it] 80%|███████▉  | 123/154 [16:55<03:55,  7.59s/it]                                                 {'loss': 0.5098, 'grad_norm': 0.2162923096324697, 'learning_rate': 1.03056077984014e-05, 'epoch': 0.79}
 80%|███████▉  | 123/154 [16:55<03:55,  7.59s/it] 81%|████████  | 124/154 [17:02<03:47,  7.59s/it]                                                 {'loss': 0.5468, 'grad_norm': 0.1902495681680097, 'learning_rate': 9.673441233863662e-06, 'epoch': 0.8}
 81%|████████  | 124/154 [17:02<03:47,  7.59s/it] 81%|████████  | 125/154 [17:10<03:41,  7.63s/it]                                                 {'loss': 0.4982, 'grad_norm': 0.4049100602619791, 'learning_rate': 9.059201449082045e-06, 'epoch': 0.81}
 81%|████████  | 125/154 [17:10<03:41,  7.63s/it] 82%|████████▏ | 126/154 [17:18<03:37,  7.78s/it]                                                 {'loss': 0.521, 'grad_norm': 1.1697772259838541, 'learning_rate': 8.463161498382948e-06, 'epoch': 0.81}
 82%|████████▏ | 126/154 [17:18<03:37,  7.78s/it] 82%|████████▏ | 127/154 [17:26<03:32,  7.87s/it]                                                 {'loss': 0.4363, 'grad_norm': 0.23690663614607674, 'learning_rate': 7.885586345533397e-06, 'epoch': 0.82}
 82%|████████▏ | 127/154 [17:26<03:32,  7.87s/it] 83%|████████▎ | 128/154 [17:34<03:22,  7.78s/it]                                                 {'loss': 0.475, 'grad_norm': 0.1915270560364483, 'learning_rate': 7.3267327459540015e-06, 'epoch': 0.83}
 83%|████████▎ | 128/154 [17:34<03:22,  7.78s/it] 84%|████████▍ | 129/154 [17:42<03:18,  7.96s/it]                                                 {'loss': 0.5463, 'grad_norm': 0.5067933639844612, 'learning_rate': 6.786849132580842e-06, 'epoch': 0.83}
 84%|████████▍ | 129/154 [17:42<03:18,  7.96s/it] 84%|████████▍ | 130/154 [17:51<03:16,  8.19s/it]                                                 {'loss': 0.4967, 'grad_norm': 0.5132883474942129, 'learning_rate': 6.266175505426958e-06, 'epoch': 0.84}
 84%|████████▍ | 130/154 [17:51<03:16,  8.19s/it] 85%|████████▌ | 131/154 [18:00<03:14,  8.47s/it]                                                 {'loss': 0.4318, 'grad_norm': 0.5011668596737769, 'learning_rate': 5.76494332489278e-06, 'epoch': 0.85}
 85%|████████▌ | 131/154 [18:00<03:14,  8.47s/it] 86%|████████▌ | 132/154 [18:08<03:02,  8.30s/it]                                                 {'loss': 0.3942, 'grad_norm': 0.40107729576013523, 'learning_rate': 5.283375408872537e-06, 'epoch': 0.85}
 86%|████████▌ | 132/154 [18:08<03:02,  8.30s/it] 86%|████████▋ | 133/154 [18:17<02:58,  8.51s/it]                                                 {'loss': 0.3816, 'grad_norm': 0.2785933376786309, 'learning_rate': 4.821685833702849e-06, 'epoch': 0.86}
 86%|████████▋ | 133/154 [18:17<02:58,  8.51s/it] 87%|████████▋ | 134/154 [18:25<02:45,  8.25s/it]                                                 {'loss': 0.529, 'grad_norm': 0.71116529734697, 'learning_rate': 4.380079838997086e-06, 'epoch': 0.87}
 87%|████████▋ | 134/154 [18:25<02:45,  8.25s/it] 88%|████████▊ | 135/154 [18:33<02:40,  8.43s/it]                                                 {'loss': 0.4079, 'grad_norm': 0.12356288131381037, 'learning_rate': 3.958753736408105e-06, 'epoch': 0.87}
 88%|████████▊ | 135/154 [18:33<02:40,  8.43s/it] 88%|████████▊ | 136/154 [18:41<02:25,  8.10s/it]                                                 {'loss': 0.4424, 'grad_norm': 0.1212120093969169, 'learning_rate': 3.557894822359864e-06, 'epoch': 0.88}
 88%|████████▊ | 136/154 [18:41<02:25,  8.10s/it] 89%|████████▉ | 137/154 [18:49<02:17,  8.09s/it]                                                 {'loss': 0.5147, 'grad_norm': 0.29429204267965425, 'learning_rate': 3.1776812947865385e-06, 'epoch': 0.89}
 89%|████████▉ | 137/154 [18:49<02:17,  8.09s/it] 90%|████████▉ | 138/154 [18:57<02:10,  8.18s/it]                                                 {'loss': 0.4627, 'grad_norm': 0.25066867452584135, 'learning_rate': 2.8182821739164534e-06, 'epoch': 0.89}
 90%|████████▉ | 138/154 [18:57<02:10,  8.18s/it] 90%|█████████ | 139/154 [19:05<02:00,  8.01s/it]                                                 {'loss': 0.5076, 'grad_norm': 0.6526279173170059, 'learning_rate': 2.4798572271356846e-06, 'epoch': 0.9}
 90%|█████████ | 139/154 [19:05<02:00,  8.01s/it] 91%|█████████ | 140/154 [19:14<01:57,  8.40s/it]                                                 {'loss': 0.3638, 'grad_norm': 0.19415967222298655, 'learning_rate': 2.1625568979651014e-06, 'epoch': 0.9}
 91%|█████████ | 140/154 [19:14<01:57,  8.40s/it] 92%|█████████▏| 141/154 [19:23<01:49,  8.44s/it]                                                 {'loss': 0.408, 'grad_norm': 0.10055253020375994, 'learning_rate': 1.8665222391821169e-06, 'epoch': 0.91}
 92%|█████████▏| 141/154 [19:23<01:49,  8.44s/it] 92%|█████████▏| 142/154 [19:31<01:41,  8.49s/it]                                                 {'loss': 0.3819, 'grad_norm': 0.10233037238741015, 'learning_rate': 1.5918848501170647e-06, 'epoch': 0.92}
 92%|█████████▏| 142/154 [19:31<01:41,  8.49s/it] 93%|█████████▎| 143/154 [19:41<01:35,  8.72s/it]                                                 {'loss': 0.3843, 'grad_norm': 0.3570081891181165, 'learning_rate': 1.338766818151982e-06, 'epoch': 0.92}
 93%|█████████▎| 143/154 [19:41<01:35,  8.72s/it] 94%|█████████▎| 144/154 [19:49<01:25,  8.54s/it]                                                 {'loss': 0.3975, 'grad_norm': 0.2160225434255983, 'learning_rate': 1.1072806644478739e-06, 'epoch': 0.93}
 94%|█████████▎| 144/154 [19:49<01:25,  8.54s/it] 94%|█████████▍| 145/154 [19:57<01:15,  8.37s/it]                                                 {'loss': 0.4061, 'grad_norm': 0.17979120288898093, 'learning_rate': 8.975292939244928e-07, 'epoch': 0.94}
 94%|█████████▍| 145/154 [19:57<01:15,  8.37s/it] 95%|█████████▍| 146/154 [20:05<01:05,  8.21s/it]                                                 {'loss': 0.4683, 'grad_norm': 0.37513784021013413, 'learning_rate': 7.096059495149854e-07, 'epoch': 0.94}
 95%|█████████▍| 146/154 [20:05<01:05,  8.21s/it] 95%|█████████▌| 147/154 [20:12<00:56,  8.10s/it]                                                 {'loss': 0.4572, 'grad_norm': 0.38932733522627394, 'learning_rate': 5.435941707156389e-07, 'epoch': 0.95}
 95%|█████████▌| 147/154 [20:12<00:56,  8.10s/it] 96%|█████████▌| 148/154 [20:20<00:47,  7.99s/it]                                                 {'loss': 0.5099, 'grad_norm': 0.5187068220346301, 'learning_rate': 3.9956775644920395e-07, 'epoch': 0.96}
 96%|█████████▌| 148/154 [20:20<00:47,  7.99s/it] 97%|█████████▋| 149/154 [20:28<00:40,  8.03s/it]                                                 {'loss': 0.4759, 'grad_norm': 0.23557270313320453, 'learning_rate': 2.77590732258326e-07, 'epoch': 0.96}
 97%|█████████▋| 149/154 [20:28<00:40,  8.03s/it] 97%|█████████▋| 150/154 [20:36<00:32,  8.09s/it]                                                 {'loss': 0.3893, 'grad_norm': 0.20206939146937, 'learning_rate': 1.7771732184357904e-07, 'epoch': 0.97}
 97%|█████████▋| 150/154 [20:36<00:32,  8.09s/it] 98%|█████████▊| 151/154 [20:44<00:24,  8.07s/it]                                                 {'loss': 0.4178, 'grad_norm': 0.37511821711330495, 'learning_rate': 9.999192295886972e-08, 'epoch': 0.98}
 98%|█████████▊| 151/154 [20:44<00:24,  8.07s/it] 99%|█████████▊| 152/154 [20:52<00:16,  8.01s/it]                                                 {'loss': 0.454, 'grad_norm': 0.34871014504029085, 'learning_rate': 4.4449087674847125e-08, 'epoch': 0.98}
 99%|█████████▊| 152/154 [20:52<00:16,  8.01s/it] 99%|█████████▉| 153/154 [21:01<00:08,  8.10s/it]                                                 {'loss': 0.4342, 'grad_norm': 0.667277016988539, 'learning_rate': 1.111350701909486e-08, 'epoch': 0.99}
 99%|█████████▉| 153/154 [21:01<00:08,  8.10s/it]100%|██████████| 154/154 [21:10<00:00,  8.46s/it]                                                 {'loss': 0.4484, 'grad_norm': 0.49305105897557416, 'learning_rate': 0.0, 'epoch': 1.0}
100%|██████████| 154/154 [21:10<00:00,  8.46s/it]                                                 {'train_runtime': 1281.4462, 'train_samples_per_second': 3.864, 'train_steps_per_second': 0.12, 'train_loss': 0.4844678152691234, 'epoch': 1.0}
100%|██████████| 154/154 [21:21<00:00,  8.46s/it]100%|██████████| 154/154 [21:21<00:00,  8.32s/it]
Rank 0:  Model saved to exp/llada_v_finetune
